[
["index.html", "APS 240: Data Analysis and Statistics with R Chapter 1 Course information and overview 1.1 Why do a data analysis course? 1.2 Course overview 1.3 How to use the teaching material 1.4 Health and safety using display screen equipment", " APS 240: Data Analysis and Statistics with R Dylan Z. Childs 2016-09-29 Chapter 1 Course information and overview This is the online course book for the Data Analysis and Statistics with R (APS 240) module. You can view this book in any modern desktop browser, as well as on your phone or tablet device. The site is self-contained—it contains all the material you are expected to learn this year. Dylan Childs is the course co-coordinator. Please email him if you have have any general queries about the course. Andrew Beckerman is the second course instructor. The Teaching Assistants (‘TAs’) this year are Ross Booton, Matthew Hethcoat, Bethan Hindle, Tamora James, Felix Lim, and Simon Mills. 1.1 Why do a data analysis course? To do science yourself, or to understand the science other people do, you need some understanding of the principles of experimental design, data collection, data presentation and data analysis. That doesn’t mean becoming a maths wizard, or a computer genius. It means knowing how to take sensible decisions about designing studies and collecting data, and then being able to interpret those data correctly. Sometimes the methods required are extremely simple, sometimes more complex. You aren’t expected to get to grips with all of them, but what we hope to do in the course is to give you a good practical grasp of the core techniques that are widely used in biology and environmental sciences. You should then be equipped to use these techniques intelligently and, equally importantly, know when they are not appropriate, and when you need to seek help to find the correct way to design or analyse your study. You should, with some work on your part, acquire a set of skills which you will use at various stages throughout the remainder of your course, in practicals, field courses and in your project work. These same skills will almost certainly also be useful after your degree, whether doing biology, or something completely different. We live in a world that is increasingly flooded with data, and people who know how to make sense of this are in high demand. The R statistical programming environment underpins much of this endeavour, in both academic and commercial settings. Learning the basic principles of data analysis with R will only improve your employment prospects. 1.2 Course overview 1.2.1 Aims This course has two main, and equal, aims. The first is to provide a basic training in the use of statistical methods and software (R and RStudio) to analyse biological data. The second is to introduce some of the principles of experimental design, sampling, data interpretation, graphical presentation and scientific writing relevant to the biological and environmental sciences. 1.2.2 Objectives By the end of the course you should be familiar with the principles and use of a range of basic statistical techniques, be able to use the R programming language to carry out appropriate analyses of biological data, evaluate your statistical models, and make sensible interpretation of the results. You should be able to relate the ways in which data are collected (by different designs of sampling or experiment) to the types of statistical methods that can be used to analyse those data. In combination with the skills you developed in APS 135, you should be able to decide on appropriate ways of investigate data graphically, be able to produce good quality scientific figures, and incorporate these, along with statistical results, into a formal report. 1.2.3 Assumed background You are assumed to be familiar with the use of personal computers on the University network, and with the use of R for data input, manipulation and plotting introduced in APS 135. If you are unsure about these basic methods, then you will need to revise the material covered in the Level 1 IT practicals. The key skills you need are covered in the Programming prerequisites chapter. We will revise the most important topics in the first practical session. Environmental Sciences Students Don’t panic if you are one of the Environmental Sciences students joining us from Geography. We’ll provide targetted help at the beginning of the course to help you learn the fundamentals of R. 1.2.4 Methods The course runs over semester 1, in weeks 1-12. The first 10 weeks consists of a 2-hour IT practical each week, along with some additional preparatory practical work and reading. All components of the course are compulsory. The remaining 2 weeks are devoted to revision and an assessed data analysis project. Students come to APS 240 with very different experiences and competancies. Because of this unavoidable situation, the course is designed as a ‘self-teaching’ module to you flexibility in the rate at which you work through the material. This doesn’t mean no one is going to help you, but you are expected to take responsibility of your own learning. 1.2.4.1 Preparatory reading and prctical work Each week starts with some preparatory reading and ‘walk-through’ practical work from the course book. We’ll let you know what you need to do each week. The book chapters generally come in one of two forms: Practical walk-through chapters. These are designed to introduce the practical aspects of different analysis techniques. These generally focus on the ‘when’ and ‘why’ we use a particular technique, as well how to actually do it in R. You are free (encouraged even) to work through these in groups. Concept chapters. These focus on ideas and concepts, rather then using R per se (though they may use R from time to time). They provide background information or more detailed discussion relating to the topics you are covering. These are an integral part of the course. Please don’t skip them! It is up to you to complete the preparatory work in your own time. However, we are not expecting you to understand everything the first time around. This point is so important it’s worth saying again—you are not expected to understand everything in the course book the first time you read it. Just do your best to understand it, taking careful notes of anything you’re struggling with. The TAs and staff will happily answer any questions you have during the timetabled sessions. Indeed, that is what they’re there to do. 1.2.4.2 Timetabled practicals The timetabled practicals take place in the APS IT rooms, either Perak IT labs or B56. In each of these sessions, you will work through a number of small exercises to help you consolidate what you’re been working on. You’ll be asked to note down your answers to some of the exercises. The correct answers will be available. TAs and staff are there to help if you get stuck, so don’t suffer in silence. You can (should!) also use this time to ask questions about concepts that are confusing you. You are welcome to use your own computer to complete your work. Keep in mind that the university computers are the only ‘officially supported’ platform. If you run into a problem using your own computer, the TAs and staff will try to help resolve these. Unfortunately, if these prove to be intractable, you will have to use the university computing facilities. It just isn’t fair on other students for teaching staff to spend valuable contact time trying to solve installation / setup problems. 1.2.5 Non-assessed material Although every topic is important, in the sense that it contains material that will help you become a better data analyst, we want to avoid creating too much of an assessment burden in this course. To this end, the material in a few of the chapters is be formally assessed. The Expected learning outcomes chapter tells you what you need to be able to know by the end of the course. If you’re the kind of person who wants to focus on the assessment, you should use that as your guide. As always, feel free ask an instructor (not a TA) for clarification if you’re not sure of what you need to be able to do. 1.2.6 What is required of you? A willingness to learn and to take responsibility for your learning! Data analysis is not the easiest subject in the world, but neither is it the most difficult. It’s worth making the effort. What you learn in this course will form the basis for much of what you do in field course, practical and project work that follows in later semesters. The minimum requirement for the course is that you: attend your designated practical session each week (please ensure that you arrive on time and register, or you will be recorded as absent), complete the preparatory practical work and reading, taking careful notes of anything that you don’t understand, be proactive about your lerning and ask the TAs and staff questions about things you’re struggling with, complete the exercises for each week before the next practical class, and check through your answers to questions using MOLE. How you work through the book is fairly flexible, but remember, the self-study preparatory work lays the foundatation for the material covered in the timetabled practicals. Take our word for it, you won’t get much out of the timetabled sessions if you skip the preparatory work. In each practical session you should aim to complete most, if not all, of the exercises for that practical. If you don’t manage to do this you should try to finish off the work in your own time before the next practical. However, if you have problems with any of the work, staff will help you during the practical sessions, even if it is not the topic designated for that session. So if you need to catch up, there will be opportunities to do so. A word of advice: Don’t let the flexibility of the course tempt you into letting a backlog of work build up. This will compromise your ability to do the assessed work when it is set and will make it difficult to revise for the exam. One last point: the university guidelines assume the total study time associated with a 10 credit module to be about 100 hours. There is an expectation that you will spend significant time outside the timetabled practical classes working on this module. You should aim to spend about 5 hours each week working through the chapters, completing the practical exercises, and finally, working on the assessed project. This leaves you about 40 hours to revise for the formal exam in the new year. 1.2.7 Assessment Assessment of the course will have two components. The first is a short data analysis project in weeks 11-12 of the course, the second is an open book exam in the winter exam period. ‘Open book’ means you will have access to this book during the exam, i.e. you don’t have to memorise everything in here, just understand it! Further details will be given as the course progresses. 1.3 How to use the teaching material 1.3.1 The online course book All of the teaching material will be made available through a single online course book (this website). The book is organised such that it forms a complete, stand-alone introduction to data analysis. You should bookmark this now if you haven’t already done so. There are a couple of very good reasons for delivering the course material this way: Practicality: Most exercises can be completed by building on the examples in the course material. Copying the relevant R code from the course website and pasting it into your script is much more efficient, and less error-prone, than copying by eye from a printed page. A website also allows us to cross-reference topics and link to the odd bit of outside reading. Permanence: Experience suggests that many of you will want to refer to the material in this course after you graduate. However, bits of paper are easy to lose, and because the R landscape is always changing, some elements of the course may be less relevant in a few years time. By putting everything on a website, we can ensure that you will always be able to access a familiar, but up-to-date data analysis course. 1.3.2 Printed material There is a small amount of printed material in this course: Cheat sheets: We will supply you with copies of the dplyr and ggplot2 cheat sheets produced by the people who build RStudio . It may help you to refer to these when you need use either the dplyr or ggplot2 packages in a practical. Assessment information: Although much of the assessment will be done on the computer, any information relating to the assessments will be produced in printed form. This will be handed out in week 10. 1.3.3 How to make best use of the teaching material DO: When working through an exercise, follow the instructions carefully, but also think about what you are doing. Work at your own pace; you are not being assessed on whether you can do an exercise in a particular time. Ask teaching staff for help in the practicals if there are things that you don’t follow, or when things don’t seem to come out the way they should—that’s what they’re there for! Collaborate! If you are not sure you understand something feel free to discuss it with a friend—more often than not this is exactly how scientists resolve and clarify problems in their experimental design and analysis. Be prepared to experiment with R to solve problems that you encounter. You can’t break your R or RStudio by generating errors. When you run into a problem, go back to the line of code that generated the first error and try making a change. Complete each week’s work before the next week’s session. You may be able to complete some sessions quite quickly, others may take more time and require more work on your own outside the timetabled periods. DON’T: Just copy what someone else tells you to do without understanding why you are doing it. You need to understand it for yourself (and you’ll be on your own in the exam). Skip practicals or preparatory work and get behind schedule — there is too much material to assimilate all at once when you get to the assessments. Like all skills data analysis is something you have practice. 1.3.4 Conventions used in the course material The teaching material, as far as possible, uses a standard set of conventions to differentiate between various sorts of information, action and instruction: 1.3.4.1 Text, instructions, and explanations Normal text, instructions, explanations etc. are written in the same type as this paragarph (obvious really), we will tend to use bold to highlight specific technical terms when they are first introduced. Italics are generally used for emphasis and with Latin names. When we want you to do something important or pay particular attentions—e.g., waksing you to write down an answer or giving you a set of instructions—we place the text inside a box like this one: Brown boxes Here is some important text telling you to do something or remember something important. Sometimes it just contains a warning that the next bit will be hard… Please don’t ignore these. At various points in the text you will also come across different coloured boxes that contain additional information: Blue boxes These aim to offer a not-too-technical discussion of how or why something works the way it does. These are things that it may be useful to know, or at least know about, but aren’t necessarily part of the main thread of a section. Red boxes These contain a warning or flag a common gotcha that may trip you up. They highlight potential pitfalls and show you how to avoid them. You will avoid a lot of future mistakes if pay close attention to these. We use block quotations to indicate an example of how a particular statistical result should be presented when you write it in a report: e.g. The mean lengths of male and female locusts differed significantly (t=4.04, df=15, p=0.001), with males being significantly larger. 1.3.4.2 R code, files and RStudio This typeface is used to distinguish R code within a sentence of text: e.g. “We use the summary function to obtain information about an object produced by the lm function.” A sequence of selections from an RStudio menu is indicated as follows: e.g. File ▶ New File ▶ R Script File names referred to in general text are given in upper case in the normal typeface: e.g. MYFILE.CSV. 1.3.5 Feedback There are a number of ways in which you can obtain feedback on how well you understand the course material 1.3.5.1 Self-assessment questions: At various points in the course material there are questions for you to answer. When you reach one of these, you should be in a position to answer the question —- so make a note of the answer! When you’ve completed the session, you can check your answers using the ‘self-test’ for that particular session on MOLE. You will see if you have the correct answer and in some cases you will also get some additional explanation as to why that answer is right (or wrong!). 1.3.5.2 Each other: Discussing what you are doing with someone as you go along, or working through a problem with someone else, can help clarify your understanding. Please bear in mind, however, that you learn little or nothing by simply copying information from someone else, and when it comes to the assessed project, it must be your own work. 1.3.5.3 Staff: In the practicals you will have opportunities to ask questions and discuss what you are doing with staff and teaching assistants. They are not just there to help you with the practical. You should use them to help you work through any problems you have with the course material, both conceptual and practical. There will also be an opportunity to have topics you raise discussed in later practicals. 1.3.6 Help sessions We will run an open ‘help’ session every Friday from 12-2.00pm, in the B56 IT Room in APS. An instructor will be on hand during this period to answer specific questions about the course material. This room holds about 40 students, so please only attend if you require one-to-one assistance, i.e, don’t just use this session to complete unfinished practicals (unless you are stuck of course). 1.3.7 Overall… We hope that the material is clear and easy to use, and that you find the course useful, or even enjoy it! In a text of this size, which is continually being improved and updated, errors do creep in; if you find something you think is wrong please tell us. If it’s not wrong we will be happy to explain why, and if it is then you will save yourself and others a lot of confusion. Similarly, if you have any comments or suggestions for improving the teaching materials please let us know. 1.4 Health and safety using display screen equipment Although using a computer may not seem like a particularly risky activity you should be aware that you can suffer ill effects if you work at a computer for long periods without observing a few sensible precautions. The standard guidelines are as follows: Make sure that your equipment is properly adjusted: ensure that your lower back is well supported by adjusting the seat back height adjust your chair seat height so that your forearms are level when using the keyboard make sure that the front edge of the keyboard is at least 8-10 cm away from the edge of the desk if you are using a mouse, have it far enough away from the edge of the desk so that your wrist is supported whilst you use it. If you can learn to use the mouse with either hand then this can help avoid strains Do not have your screen positioned in such a way that there is glare or reflections from the windows or room lights on the screen. Maintain good posture. Take regular breaks away from the computer. It is recommended that you take about 10 minutes break every hour. Most Departments will have a Display Screen Trainer or Advisor, who can offer specific advice if you are using a display screen for a substantial amount of time, or if you experience, or anticipate, specific problems. "],
["expected-learning-outcomes.html", "Chapter 2 Expected learning outcomes 2.1 Statistical Concepts (I)", " Chapter 2 Expected learning outcomes 2.1 Statistical Concepts (I) Given a description of some data, classify variables as numeric vs categorical, ratio vs. interval, and ordinal vs. nominal. Construct simple summaries of numeric variables in R using the mean, sd, var functions. Explain what sampling error is (in non-technical terms) and understand why it is necessary to quantify sampling error alongside point estimates. Recognise the difference between the distribution of a sample, and the sampling distribution of an estimate derived from that sample. Recognise the difference between the standard deviation (a property of a sample) and the standard error (a property of a sampling distribution). Calculate the standard error of a sample mean when the population distribution of a variable follows a normal distribution. Given a description of an experimental setting, be able to recognise… …the difference between a statistical population and a sample from the population. …the difference between a population parameter and a point estimate of the population parameter. You are not expected to be able to explain or use the bootstrap or permutation tests—these were introduced to help you learn the principles listed above. "],
["programming-prerequisites.html", "Chapter 3 Programming prerequisites 3.1 Starting an R session in RStudio 3.2 Using packages 3.3 Reading data into R 3.4 Data frames 3.5 Package functions", " Chapter 3 Programming prerequisites This chapter gives a quick overview of the prerequisite R skills needed for this course (we studied these last year). We will use these skills this year, so you may need to spend revising them if you feel that you’re a little rusty. Biology students The key R skills you need to have in place to work through this book will be revised in the first practical session. The lecture slides will be placed on MOLE after the practical. You can also access a version without the answers to exercises here. Environmental Science students If you are an Environmental Science student joining us from Geography the material in this section won’t make any sense to you at the moment. Don’t panic! We will help you catch up in the first few weeks of the course, and because you will be working in a smaller group, you will be able to access more one-to-one help. 3.1 Starting an R session in RStudio Here is a quick overview of the process you should go through every time you start a new R session: Open up RStudio and set your working directory. You should do this via the RStudio menu system: Session ▶ Set Working Directory ▶ Choose Working Directory…. Make sure that you choose a sensible location. This is where you will store your data and R scripts, so it needs to be somewhere you can find and access again each time you use R. If you want to keep life really simple, it is a good idea to use the same location in every practical, but you don’t have to do this. Open up a new R script using the RStudio menu system: File ▶ New File ▶ R Script. Don’t create any other kind of file. There are a couple of things that need appear at the start of every script. Add these to the top of your new script before you do anything else. You should always clear the workspace with rm, and load up any packages you plan to use: # clear the workspace so that we have a &#39;clean sheet&#39; rm(list = ls()) # load and attach the packages we want to use... # 1. &#39;dplyr&#39; for data manipulation library(dplyr) # 2. &#39;ggplot2&#39; for plotting library(ggplot2) Now run the preamble section of the script, i.e. highlight everything and hit Ctrl+Enter. If the library commands didn’t work it suggests that you have not previously installed the relevant package. Install the package (see below) and try rerunning the script. Once the preamble bit of the script is working you should save the script. Look at the label of the tab the script lives in. This will probably be called something like Untitled1, and it the label will be red. This is RStudio telling you that you have not saved the file yet. You are now ready to start developing your new script. 3.2 Using packages R packages extend the basic functionality of R so that you can do more with it. A package bundles together R code, data, and documentation in a way that is easy to use and share with other users. Last year we learned how to use some of the functions provided by the dplyr package (for data manipulation) and the ggplot2 package (for making plots). We are going to use the dplyr and ggplot2 packages again this year, so you need to understand R’s package system in order to access these. You can revise how to use the package system in the packages topic. It isn’t difficult to use, and we will obviously help you if you run into difficulties. Installing a package is done via the install.packages function, e.g. install.packages(&quot;dplyr&quot;) Loading and attaching the package a package happens via the library function, e.g. library(&quot;dplyr&quot;) The key point—which seems to cause endless confusion—is that installing a package, and then loading and attaching the package, are different activities. You only have to install it once onto your computer, but you have to load a package every time you want to use it in a new R session (i.e. every time you start up RStudio). 3.3 Reading data into R Last year we made extensive use of several data sets that reside inside various R packages. This was useful because it meant we could use the data without first reading it into R, meaning that we could concentrate on developing your R skills rather than fixing data input errors. We don’t have the luxury of doing this when we work with our own data, and so this year, we will adopt more realistic practices. Whenever you need to work with a data set, you will have to first download it (from MOLE), and then read it into R. Each data set is stored as a Comma Separated Value (‘CSV’) text file, and so you will need to use the read.csv function to read it in. You can revise how all of this works in the relevant section of the data frames topic. 3.4 Data frames When you read data into R using a function like read.csv, it places that data into a data frame. The data frame is the most important type of object in R. It is table-like object that collects together different variables, storing each of them as a named column. We can access the data inside a data frame by referring to particular columns and rows. You can revise how to work with data frames in the data frames topic. 3.5 Package functions We will use functions from the dplyr package from time-to-time to manipulate data, and we will use the ggplot2 package to make plots of our data and summarise statistical models. However, we will remind you which functions you need to use to solve a particular problem as the course unfolds, so there is no need to revise all of this material now. "],
["the-scientific-process.html", "Chapter 4 The scientific process 4.1 Stages in the scientific process 4.2 Hypothesis testing 4.3 Don’t we ever know anything for sure? 4.4 Further reading", " Chapter 4 The scientific process There is something fascinating about science. One gets such a wholesale return of conjecture out of a trifling investment of fact. Mark Twain To do science is to search for patterns, not simply to accumulate facts. Robert MacArthur 4.1 Stages in the scientific process Science is about asking, and answering, the right questions. Within this process a number of distinct stages usually occur: making observations, asking questions, formulating hypotheses, and testing predictions. Collectively these are the building blocks of what is known as the scientific method. Exactly how they fit together, and what the philosophical and practical limitations of different approaches are, have been the subject of much debate by philosophers of science. We are not going really going to tackle those issues here, fascinating though they are, but instead try to extract a general working framework for the process of a typical scientific investigation. 4.1.1 Observations Observation — information, or impression, about events or objects. In general the questions we ask are not generated by pure abstract thought, but are a result of observations about the natural world. These may take the form of direct observations we make ourselves, patterns that crop up in data collected for other purposes, in non-specific surveys, and the previous work, or accumulated information, of other people. So, while pottering around in a stream one day, you notice that the freshwater ‘shrimps’ (Gammarus) that abound in the stream seem to occur almost entirely under stones; you rarely seem to see them when you just watch a patch of open stream bed. Having made observations, it may be necessary to collect some more data to check that this phenomenon is not just a one-off event, or a false impression. Look under a few more stones, watch the same species another day, or in another place, check the literature for similar observations by others. Such observations of biological systems will lead almost automatically into asking questions. 4.1.2 Questions Question — what it is that you want to know; the scope of your investigation. e.g., Why does Gammarus spend most of its time under stones? You should try and make your question reasonably focused - the overall aim of your study is to answer this question. The question of why the tropics are more diverse than the temperate regions is a vast topic - so even though it is a valid (and fascinating) question, it may not be a good choice for a final year project or even a PhD! The next stage is to formulate an hypothesis. 4.1.3 Hypotheses Hypothesis - an explanation proposed to account for observed facts. In general, in biology, the important distinguishing feature of an hypothesis is that it specifies some biological process, or processes, which might account for the observations made. One question will often generate more than one hypothesis: Gammarus occur under stones because: they need to shelter from the current their food (leaf litter) gets trapped and accumulates under stones they are subject to predation by visually hunting fish and need to remain out of sight Formulating hypotheses requires more than just a restatement of the question - it usually embodies some mechanism (though in some cases this may not be fully understood) and it will often draw on additional information (e.g., the fact that Gammarus feed on dead leaves). 4.1.4 Predictions Prediction — what you would expect to see if the hypothesis was true. Hypotheses are about proposing explanations, but they might not be directly testable; that is they may not tell you what data to collect, or what pattern to expect in the data. To be able to test an hypothesis you need to make some predictions from that hypothesis. These will be determined both by what you expect to see and what it is possible, or practical, to measure. A prediction is not simply a rephrasing of the hypothesis - it should more or less give you a statement of the experiment to conduct or observation to make, and type of data to collect: Shelter hypothesis: a greater proportion of Gammarus should be found in the open in streams with slow flow, or in slower flowing areas of a stream. Food hypothesis: Gammarus should not aggregate under stones from which all leaf litter has been removed; Gammarus should aggregate on patches of leaf litter tethered in the open part of the stream bed. Predation hypothesis: Gammarus should aggregate under stones more in streams where fish are present than where they are not; Gammarus may spend less time under stones at night. Ideally you are looking for a prediction that is unique to the hypothesis it is based on - so if the prediction is true only one of the hypotheses could have been responsible, but this may not always be possible and some combination of predictions may need to be used. Additionally, several processes may be operating at the same time. This makes hypothesis testing harder still. It may be necessary to consider two or more hypotheses, and their corresponding predictions, in combination. For example, Gammarus may be under stones because it prefers the sheltered environment, but also because food accumulates there. In this case we might expect that Gammarus will show a weak aggregative response to shelter alone, or food alone, and a stronger one to them both together. 4.2 Hypothesis testing Once we have firmed up our questions, hypotheses, and predictions we can collect the data to evaluate our ideas. On the basis of these data we will either accept or reject the various hypotheses. The important thing to realise about the process of hypothesis testing is that, in science especially, hypotheses are either rejected, or not rejected, but an hypothesis can rarely, except in trivial cases, be proved. This seems like an odd state of affairs! True, but it does make sense. Since you cannot be sure that you have thought of all the possible hypotheses to explain an observation, finding evidence that supports the prediction from one your hypotheses, does not guarantee that the hypothesis is the only one which could have produced the effect you find. On the other hand, if you find evidence that directly contradicts the prediction(s) from your hypothesis, you can be certain (assuming the prediction and data are not flawed) that the hypothesis cannot be true. An hypothesis which predicted that all conifers should be evergreen could be supported by numerous observations of different conifer species in forests around the world, but is conclusively refuted by the first larch tree we encounter. Having tested your hypothesis, by examining the evidence that its predictions are true, you may accept it as the best (so far) explanation of the observations, or you may reject it as an explanation, and turn to other hypotheses. The same procedure must then be repeated for these hypotheses. This basic cycle of proposing hypotheses and then seeking evidence potentially capable of falsifying them, is, in essence, the idealized model of the scientific process famously proposed by the philosopher of science Karl Popper (1902-1994). It is often termed falsificationism. 4.3 Don’t we ever know anything for sure? The method presented here provides a view of science as one in which we suggest hypotheses, then test them trying to reject them by finding conclusive counter-evidence, then replacing them with new hypotheses. It all sounds a bit frustrating. In fact of course we do ‘accept’ hypotheses all the time — that is we fail to reject them over and over again. These hypotheses become more accepted and in some sense become regarded as ‘true’ if repeated attempts to test them all fail to provide good counter evidence. In other words, we have some ideas that are doing pretty well in terms of resisting falsification, and we use these as our best estimates of the truth, with the proviso that it is still possible a better idea will come along in due course. The simple process of falsification described above also presents a picture of scientists as wonderfully neutral, objective creatures, rationally proceeding through cycles of setting up hypotheses, testing them, rejecting them, cheerfully setting them aside and starting over again. Of course this is not a true reflection of the complex, often messy, business really involved in trying to figure out how the world works. Philosophers of science have argued long and hard about how far from this idealized process real science actually is. Various alternative philosophies suggest more ‘realistic’ processes, such as Thomas Kuhn’s view of science as periods of relative stasis, where people work within an accepted paradigm (a set of views about how things work) despite accumulating evidence that doesn’t always support the paradigm, until finally it is upset by a ‘revolution’ which rejects the entire paradigm, and proposes a new view. The philosopher Imre Lakatos proposed some resolution of these views, suggesting that scientific ideas were grouped together in ‘research programmes’ concerned with particular endeavours, and that within these there may be core ideas that are not challenged, but other related ideas which are being challenged and adjusted by falsification, and that together these make each research programme progress. Programmes that don’t progress should be abandoned in favour of those that do. Of course that is a very over-simplified sketch of some important ideas, which are well worth reading a bit about, but in practice these philosophical arguments are really more focused on how whole areas of science develop. When you are just thinking about constructing a simple study of one problem, then the basic falsification cycle is a pretty good approach to have in your mind. Even in it’s simple form, however, it is not immune from the effect of human fallibility (see below). The process laid out here is not a strict set of rules, but outlines an approach to scientific investigation which is widely considered to provide a rigorous and productive system. As with all such systems understanding the ’normal’ process is a prerequisite for constructively breaking the rules. It’s hard to reject an hypothesis you love! An interesting aside on the process is given by Sutherland (1994) who suggests that, in everyday life at least, we are often very reluctant to deliberately seek evidence that might refute a hypothesis we have formulated, and often persist in holding on to the hypothesis even when the evidence is against us. This can be seen in experiments where people are presented with a sequence of numbers (say 2, 4, 6) and have to try and establish the general rule to which the set of numbers conforms. The subject decides on an initial rule and then can test this rule by suggesting other sets of numbers and being told whether or not those numbers conform to the rule. The initial guess at a rule is usually ‘even numbers in ascending order’, and the initial test suggestions are typically another set of even numbers ascending by two (e.g. 16, 18, 20). These, the subjects are told, conform to the rule. The next set of numbers suggested is then often another similar set (40, 42, 44) – which again conforms, and sometimes another similar guess will follow. However, the suggestion of further sets of even numbers ascending by two cannot test this (most simple rules that allow 2, 4, 6 will allow the other sequences too). A better first suggestion would be to change one of the components of the initially proposed rule e.g. even numbers to odd numbers: 3, 5, 7; or the increment: 2, 8, 14. If these conform we can reject a specific component of our initial rule. (The rule is, in fact, simply any ascending sequence of numbers.) 4.4 Further reading Barnard C, Gilbert F and McGregor P (1993) Asking questions in biology. Longman. Ladyman, J (2002) Understanding the philosophy of science. Routledge. Sutherland S (1994) Irrationality. Penguin. "],
["data-and-variables.html", "Chapter 5 Data and variables 5.1 “Observations on material and obvious things” 5.2 Revision: Types of variable 5.3 Accuracy and precision", " Chapter 5 Data and variables The truth is the science of nature has been already too long made only a work of the Brain and the Fancy. It is now high time that it should return to the plainness and soundness of Observations on material and obvious things. Robert Hooke (1665) The plural of anecdote is not data. Roger Brinner 5.1 “Observations on material and obvious things” As Hooke’s observation suggests, science cannot proceed on theory alone. The information we gather about a system both stimulates questions and ideas about it and, in turn, can also allow us to test these ideas. In fact the idea of measuring and counting things is so familiar to us that it is easy to start a project without giving much thought to something as apparently mundane as the nature, quantity and resolution of the data we intend to collect. It is worth considering, however, as features of the data in a study determine both the types of analyses that can be carried out, and the confidence we can have in any conclusions that are drawn. We will spend quite a lot of time considering the statistical tools that can help us extract information from data, but no statitical wizardry can extract information that isn’t there to begin with. So what is there to say about data? The first point to note is that, properly, the word data is the plural of datum (a single, often numerical, piece of information) …so we should say “the data are…” not “the data is…”. However, the use of the word in the singular is becoming widespread, and you will commonly hear it used in this way. Grammar Nazis don’t like this though, so it’s worth knowing what the “correct” subject-verb agreement looks like if you want to avoid incurring their wrath. The second point is that there are many different sorts of data. Examples include spatial maps of the occurance of a particular species and environmental variables, DNA sequences or even the whole genomes of individuals, and networks of feeding relationships among species (i.e. food webs). These kinds of data can be very challenging to analyse correctly. Fortunately for us, we are concerned with relatively simple kinds of data in this course. When we collect data it is typically organised as a set of one or more related statistical variables. Remember, what we learned past year. Statisticians use the word ‘variable’ as a generic term to refer to any characteristic that can be measured or experimentally controlled on different items or objects. We tend to think of variables as numeric quantities, but there is nothing to stop us working with non-numeric variables. Collectively, a set of related variables are referred to as a data set (or just ‘the data’). Confused? Let’s look at a concrete example. Consider the spatial map example above. A minimal data set might comprise two variables containing the x and y position of sample locations, a third variable denoting the presence / absence of a species, and one or more additional variables containing information about the environmental factors we measured. Data and variables in R Remember what you learned last year about data frames and vectors? When using R, we typically store a data set in a data frame. Each column in the data frame is one of R’s vectors — numeric, character, etc. Remember the ‘tidy data’ concept from last year? If the data are tidy then the columns of the data frame should correspond to the statistical variables in our data, and each row corresponds to a single observation. This simple connection between abstract statistical concepts and the concrete objects in R is not coincidence—R was designed first and foremost to analyse data. 5.2 Revision: Types of variable Again, because we handle data of one sort or another so frequently, we often don’t stop and think about exactly what kind of data we are using. Most of the time that doesn’t cause too much of a problem. However, when you come to design your own studies, and analyse your own data, it can be very important to understand what sort of data you need, or have, as it can affect what information you can extract from it. Last year we learned that the variables that comprise a data set can be classified as being either numeric or categorical: categorical variables have values that describe a characteristic of an observation, like ‘what type’ or ‘which category’; numeric variables have values that describe a measurable quantity as a number, like ‘how many’ or ‘how much’. Categorical variables can be further characterised according to whether or not they have a natural order (nominal vs. ordinal variables), and numeric variables can be further characterised according to the type of scale they are meaured on (interval vs. ratio scales). Let’s review these classifications. 5.2.1 Nominal (categorical) variables Nominal variables arise where observations are recorded as categories which have no natural ordering relative to each other. For example: Marital status Sex Colour morph Single Male Red Married Female Yellow Widowed Black Divorced Data of this type are common in surveys where, for example, a record is made of the species found at each site. 5.2.2 Ordinal (categorical) data Ordinal variables occur where observations can be assigned some meaningful order, but where the exact numerical relationship between items in the order are not necessarily fixed, the same, or even known. For example If you are studying the behaviour of an animal when it meets another individual it may not be possible to obtain quantitative data about these interactions, but you can score the behaviours you see in order of aggressiveness: Behaviour Score initiates attack 3 aggressive display 2 ignores 1 retreats 0 Rank orderings are also ordinal data. For example the order in which runners finish a race (1st, 2nd, 3rd, etc..) is a rank ordering it doesn’t tell us whether it was a close finish or not, but still conveys important information about the result. In both situations you can say something about the relationships between categories: in the first example, the larger the score the more aggressive the response; in the second example the greater the rank the slower the runner. However, you can’t say that the gap between the first runner and the second was the same as between the second and third (even though 2-1=3-2) and you can’t say that a score of 2 is twice as aggressive as a score of 1. How should you code different categories? We always have to define some kind of coding scheme to represent the different categories of a nominal/ordinal variables. It was once common practise to assign numbers to different categories (e.g. Female=1, Male=2) for handling data in computerised form. This method was sensible in the early days of computer-based data analysis because it allowed data to be stored efficiently—numbers take up less space in memory than words. However, this efficiency argument is much less relevant on a modern computer with many Gb of memory. There are good reasons to avoid numeric coding schemes though: Numeric coding makes it harder to understand your raw data and to interpret the output of a statistical analysis of those data, because you have to remember which number is associated with each category. This is particularly problematic when a variable has many categories. Numeric codes are arbitrary. This means, for example, they should not be treated as numbers for mathematical operations (it is meaningless to say 2 [“male”] is larger than 1 [“female”]). R has a special way of representing categorical variables (called ‘factors’), so it assumes that any variable containing numeric values is meant to treated as a number. So here’s the warning: always use words (e.g., ‘female’ vs. ‘male’), not numbers, to describe the different categories when you are preparing your data for analysis in R. You are much more likely to make a silly mistake if you don’t do this, as R will try to treat the offending categorical variable as a number. 5.2.3 Interval scale (numeric) variables Interval scale varaibles take values on a consistent numerical scale but where that scale starts at an arbitrary point. Temperature on the Celsius scale is a good example of interval data. You can say that 60\\(^{\\circ}\\)C is hotter than 50\\(^{\\circ}\\)C. You can also say that the difference in temperature between 60\\(^{\\circ}\\)C and 70\\(^{\\circ}\\)C is the same as that between -20\\(^{\\circ}\\)C and \\(-10^{\\circ}\\)C. However you cannot say that 60\\(^{\\circ}\\)C is twice as hot as 30\\(^{\\circ}\\)C because temperature on the Celsius scale has an artificial zero value (the freezing point of water). This point becomes obvious when you consider that temperature can equally well be measured on the Fahrenheit scale (where the freezing point of water is 32 degrees). There is a temperature scale which has a true zero: the Kelvin scale. Zero K is absolute zero, where a substance actually has no thermal energy whatsoever. So temperature in degrees K would not be interval data. You can add and subtract data measured on an interval scale but you cannot divide or multiply such data (and get a meaningful result). 5.2.4 Ratio scale (numeric) variables Ratio scale variables have a true zero and known and consistent mathematical relationship between any points on the measurement scale. Temperture measurements in degrees K are on a ratio scale, i.e. it makes sense to say that 60 K is twice as hot as 30 K. These are the variables we are most used to, because physical quantities are often measured on a ratio scale. For example, length, weight, or numbers of organisms are usually measured on a ratio scale. You can add, subtract, multiply and divide this sort of data and get meaningful results. Continuous or discontinuous? A common confusion with numeric data concerns whether the data are on continuous or discontinuous scales. Ratio data can be either. Many biological ratio data are discrete (i.e. only certain discrete values are possible in the original data), and therefore discontinuous. Count data are an obvious example, e.g. the number of eggs found in a nest, the number of plants recorded in a quadrat, or number of heartbeats counted in a minute. These can only comprise whole numbers, ‘in between’ values are not possible. However, the distinction between continuous and discontinuous data is often not clear cut – even ‘continuous’ variables such as weight are made discontinuous in reality by the fact that our measuring apparatus is of limited resolution (i.e. a balance may weigh to the nearest 0.01 g). So… just keep in mind that the fact that data look (or really are) discontinuous does not mean they are necessarily ordinal data. 5.2.5 Which is best? All types of data can be useful but it is important to be aware that not all types can be used with all statistical models. This is one very good reason for why it is worth having an idea of the statistical tools you intend to use when designing your study. In general, ratio data is the data type best suited for statistical analysis. But biological systems often cannot be readily represented as ratio data, or the work involved in collecting good ratio data may be vastly greater than the resources allow, or the question we are interested in may not demand ratio data to achieve a perfectly satisfactory answer. It is this last question that should really come first when thinking about a study. What sort of data do we need to answer the question we are interested in? If it is clear at the outset that data on a rank scale will not be sufficiently detailed to enable us to answer the question then we must either develop a better way of collecting the data, or abandon that approach altogether. If you know the data you are able to collect cannot address the question, then you would be better doing something else, so it is good to work that out in advance. And an obvious, but important point: you can always convert measurements taken on a ratio scale to an interval scale, but you cannot do the reverse. Similarly, you can convert interval scale data to ordinal data, but you cannot do the reverse. In general, it is a good idea to avoid such conversions if you can, as they inevitably result in a loss of information. 5.3 Accuracy and precision 5.3.1 What do they mean? The two terms accuracy and precision are used more or less synonymously in everyday speech, but in scientific investigation they have quite distinct meanings. Accuracy – how close a measurement is to the true value of whatever it is you are trying to measure. Precision – how repeatable a measure is, irrespective of whether it is close to the actual value. If you are measuring an insect’s weight on an old and poorly maintained balance, which measures to the nearest 0.1 g, you might weigh the same insect several times and each time get a different weight — the balance is not very precise, though some of the measurements might happen be quite close to the real weight. By contrast you could be using a new electronic balance, weighing to the nearest 0.01g, but which has been incorrectly zeroed so that it is 0.2 g out from the true weight. Repeated weighing here might yield results that are identical, but all incorrect (i.e. not the true value) — the balance is precise, but the results are inaccurate. The analogy often used is with shooting at a target: Figure 5.1: Accuracy and precision It is obviously important to know how accurate and how precise your data are. The ideal is situation in the top left target in the diagram, but in many circumstances high precision is not possible and it is usually preferable to make measurements of whose accuracy you can be reasonably confident (bottom left), than more precise measurements, whose accuracy may be suspect (top right). Taking an average of the values for the bottom left target would produce a value pretty close to the centre; taking an average for the top right target wouldn’t help your accuracy at all (though the repeatability of the values might well give you spurious confidence in the data). It is also worth being aware that when you state results, you are making implicit statements of the precision of the measurement. 5.3.2 Implied precision - significant figures The number of significant figures you use suggests something about the precision of the result. A result quoted as 12.375 mm implies the measurement is more precise than one quoted as 12.4 mm. A value of 12.4 actually measured with the same precision as 12.735 should properly be written 12.400. When quoting results look at the original data to decide how many significant figures to use - generally the same number of significant figures will be appropriate. If you are working with discrete data these considerations do not apply in quite the same way, e.g. precision of measurement is not an issue in recording the number of eggs in a nest. You use 4 not 4.0, but since 4 eggs implies 4.0 eggs you would be correct to quote average clutch size from several nests as 4.3 eggs. However, even with discrete data, if numbers are large then obviously precision is an issue again … a figure of 300 000 ants in a nest is likely to imply a precision of plus or minus 50 000. A figure of 320987 ants implies a rather improbably precise measurement (nobody will believe you actually counted them all!). 5.3.3 How precise should measurements be? The appropriate precision to use when making measurements is largely common sense. It will depend on practicality (it may not be possible to weigh an elephant to the nearest 0.001g) and the use to which you wish to put the data (if you want to know whether the elephant will cause a 10 tonne bridge to collapse then the nearest tonne will be good enough, if you want to compare the mean sizes of male and female elephants then the nearest 100 kg may be sufficient, if you want to monitor the progress of a pregnant female elephant then the nearest 10 kg or less might be desirable). As a rough guide aim, where possible, for a scale where the number of measurement steps is between 30 and 300. So for example, in a study of the variation in shell thickness of dogwhelks on a 300 m transect up a shore, it would be adequate to measure the position of each sampling point on the transect to the nearest metre, but shell thickness will almost certainly need to be measured to the nearest 0.1 mm. 5.3.4 Error, bias and prejudice Error is present in almost all biological data, but not all error is equally problematic. Usually the worst form of error is bias. Bias is a systematic lack of accuracy, i.e. the data are not just inaccurate, but all tend to deviate from the true measurements in the same direction (situations B and D in the ‘target’ analogy above). Thus there is an important distinction in statistics between the situation where the measurements differ from the true value at random and those where they differ systematically. Measurements lacking some precision, such as the situation illustrated in C, may still yield a reasonable estimate of the true value if the mean of a number of values is taken. Avoiding bias in the collection of data is one of the most important skills in designing biological (or other) investigations. Some forms of bias are obvious, others more subtle and hard to spot. Some sources of bias in biology include: Non-random sampling. Many sampling techniques are selective, and may result in biased information. For example pitfall trapping of arthropods will favour collection of the very active species, which encounter traps most frequently. Studying escape responses of an organism in the lab may be biased since the process of catching organsims to use in the study may have selected for those whose escape response is poorest. Conditioning of biological material. Organisms kept under particular conditions, especially in a laboratory, for periods of time may become acclimatised to conditions unlike those they normally encounter, or if kept in a laboratory for many generations characteristics may change through natural selection. Such organisms may give a biased impression of the behaviour of the organism in natural conditions. Interference by the process of investigation. Often the process of making a measurement itself distorts the characteristic being measured. For example it may be hard to measure the level of adrenalin in the blood of a small mammal, without affecting the adrenalin level in the process. Pitfall traps are often filled with a preservative, such as ethanol, but the ethanol attracts species of insect that normally feed on decaying fruit and use the fermentation products as a cue to find resources. Investigator bias. Measurements can be strongly influenced by conscious or unconscious prejudice on the part of the investigator. We rarely undertake studies without some initial idea of what we are expecting, or we form ideas about the patterns we think we are seeing as the study progresses. This can introduce bias. For example, rounding up ’in between’ values in the samples you are expecting to have large values and rounding down where a smaller value is expected, or having another ’random’ throw of a quadrat when it doesn’t land in a ’typical’ bit of habitat. The ways in which biases, conscious and unconscious, can affect our investigations are many, often subtle, and sometimes serious. Sutherland (1994) gives an illuminating and sometimes frightening catalogue of the ways in which biases affect our perception of the world and the judgements we make about it. The message is that the results you get from your investigation must always be judged and interpreted with respect to the nature of the data that were used to derive them – if the data are suspect, then the results will be suspect too. "],
["learning-from-data.html", "Chapter 6 Learning from data 6.1 Populations 6.2 Learning about populations 6.3 A simple example", " Chapter 6 Learning from data Statistics is the science of learning from data, and of measuring, controlling, and communicating uncertainty; and it thereby provides the navigation essential for controlling the course of scientific and societal advances Davidian and Louis (2012) The particular flavour of statistics we use in this course is called ‘frequentist statistics’. It isn’t hugely important that you remember that phrase, or indeed, from a practical perspective, that you even know you’re using frequentist statistics. You can apply the tools by just learning a few besic ‘rules’. Nonetheless, if you’re the kind of person who likes to understand how things properly, it is useful to at least get a rough sense of how frequentist ideas works. The goal of this, and the next few chapters, is to provide such an overview. We’re going to avoid all of the challenging mathematics that underlies this, and try to focus instead on the important concepts. That doesn’t mean the ideas are simple. It’s not critical for all of this to make perfect sense. You certainly won’t be assessed on your ability to explain how frequentist statistics works. However, if you can wrap your head around the core ideas you will find it easier to understand the output from the various statistical tests we’ll learn later. We’re going to start, in this chapter, by laying out a somewhat simplified overview of the steps involved in ‘doing frequentist statistics’. We’ll also introduce a few key ideas and definitions along the way. Later chapters will drill down into the really important ideas—things like sampling variation, standard errors, null hypotheses and p-values. These are the concepts we really need to understand. 6.1 Populations When a biologist talks about a population they mean a group of individuals of the same species who interbreed. This definition, or at least something similar, should be familiar to you. What does a statistician mean when they talk about populations? The word has a different meaning in statistics. Indeed, it is a much more abstract concept: a statistical population is any group of items that share certain attributes or properties. This is best understood by example… The readers of this book could be viewed as a statistical population. APS students have a common interest in biology, they are mostly in their late teens and early 20s, and they tend to have similar educational backgrounds and career aspirations. As a consequence of these similarities, APS students tend to be more similar to one another than they would be to a randomly chosen inhabitant of the UK. The different areas of peatland in the UK comprise a statistical population. There are many peatland sites in the UK, and although their ecology varies somewhat from one location to the next, they are also very similar in many respects. For example, all peatland is generally characterised by low-growing vegetation (blank bog, etc) and acidic soils. If you visit two different peatland sites in the UK, they will seem quite similar compared to, for example, a neighbouring calcareous grassland (think of the Peak district). A population of plants or animals—as understood by biologists—can also be thought of as a statistical population. Indeed, this is often the kind of population organismal biologists are most interested in. The individuals that comprise a biological population share common behaviours, physiology and life history characteristics. Much of organismal biology is concerned with learning about these properties of organisms, often with the goal to explaining the variation we see among individuals. Populations are conceptualised as fixed but unknown quantities within the framework of frequentist statistics. The goal of an analysis is to learn something about populations by collecting data. Note that ‘the population’ is defined by the investigator, and the ‘something we want to learn about’ is anything we’re interested in and know how to measure. Consider the examples again. A social scientist might be interested in understanding the political attitudes of undergraduates, so they might choose to survey a group of students in their university. A climate change scientist might measure the mass carbon is stored in peatland areas at sites across Scotland and northern England. A behavioural ecologist might want to understand how much time beavers spend foraging for food, so they might study one of the two Scottish populations. What are the steps involved in these kinds of studies? 6.2 Learning about populations The examples discussed above involve very different kinds of populations and questions. Nonetheless, there are fundamental commonalities in how these questions are addressed, which involve collecting data and applying the appropriate statistical tools. The process can be broken down into a number of distinct steps: Step 1: Refine your questions, hypotheses and predictions This step was discussed in The scientific process chapter so there’s no need to go over it again here. The key point to keep in mind is that we should not start collecting data until we’ve set out the relevant questions, hypotheses and predictions. This might seem blindingly obvious, but it is surprising how often people don’t get these things straight before diving into data collection. Take our word for it, collecting data without a clear scientific objective and rational for the work is a guaranteed way to waste your time. Step 2: Decide which population(s) is (are) important The second step is to decide which population (or populations) we need to study. This is a more subtle problem than you might think. What constitutes ‘the population’ might be fairly obvious in some kinds of study, e.g. observational studies that don’t involve an experimental approach. In each of the three cases considered above, the corresponding populations we choose to study could be undergraduate students in APS, peatland habitats from across the UK, and beavers in Scotland, respectively. But what happens if we’re planning an experiment? Imagine we want to test the prediction that nutrient addition reduces biodiversity in chalk grasslands. We could set up an experiment where we have two kinds of plots: 1) manipulated plots where we add fertiliser, and 2) control plots where we do nothing. Comparing these would allow us to assess the impact of adding nutrients on biodiversity. There are two statistical populations in this setting—control and manipulated communities, which are defined by the experimental design we adopted. The nutrient addition plots doesn’t exist until you do the experiment, and even then, we want to be able to generalise our results beyond the one experiment. The weird mental contortion that a frequentist does is to imagine that the experimental plots are part of some larger, unobserved population of nutrient addition plots. Don’t worry too much if that is confusing (it is!). The important point is that, for any given problem, a relevant statistical population is something the investigator defines. It might be ‘real’, like the undergraduates in APS, or they might be something that doesn’t even exist in a meaningful way, like a population of not-yet-realised experimentally manipulated plots. In either case, we can use the same statistical techniques to learn about ‘the populations’. Step 3: Decide which variables to study The next step is to decide which features of the population we need to measure to address our question. In practise, this comes down to deciding which variable (or variables) we need to measure. In the examples above, the appropriate variables might be things like a standardised measure of political attitude, the mass of carbon stored per unit area, or the body mass of individuals in the biological population. This step is often reasonably straightforward, though some effort may be required to pick among different options. There isn’t a whole of ambiguity associated with a physical variable like body mass, but something like ‘political attitude’ needs careful thought. Can we quantify this by studying just one thing, like voting patterns? Probably not. Part of the art of designing a good data collection protocol is deciding what to measure. We discussed some of the considerations in the Data and variables chapter, but what really matters most is that we choose the right kind of variables to address the substantive research question. Step 4: Decide which population parameters are relevant Once we have decided which variable(s) to study, we have to decide which ‘population parameter’ is relevant. A population parameter is simply a numeric quantity that describes a particular aspect of the variable(s) in the population. Actually, to be more precise, it describes a feature of the distribution of the variable(s) in the population. A simple population parameter you are familiar with is the population mean. We often study means, because they allows us to answer questions like, “how much, on average, of something is present?”. Much of this course is about asking questions of population means, though other population parameters may also be important, e.g. The goal of statistical genetics is to partition variablity among individuals—we want to know how much phenotypic variation is due to genetic vs. non-genetic sources. In this case, it is population variances we want to learn about. Sometimes we want to understand how two or more aspects of the population are related to one another. In this situation a correlation coefficient (more about this later) might be the right population parameter to focus on. Step 5: Gather a representative sample If we could measure every object in a population we wouldn’t need to use statistics. We could just calculate the quantity we needed using an exhaustive sample and we’d have our answer. In the real world we are faced with resource constraints, i.e. we have limited time and money to invest in a problem, no matter how important it is. This means we have to work with a sample of a population. A sample is just a subset of the wider population, which has been chosen so that it is representative of that population. That word ‘representative’ is very important. If we can’t collect a representative sample it will be very difficult to infer anything useful about the population it came from. For example, if we aim to understand the reproductive characteristics of our favourite study organism, but we only sample young or old individuals, it will be impossible to generalise our findings if reproductive performance changes with age (which is almost always true). The study of how to generate useful samples from a population is an important part of statistics. It falls under the banners of experimental design and sampling theory. These are large, quite technical topics, so it is well beyond the scope of this course to study them in any great deal. Nonetheless, we will touch on a few of the more important practical aspects as we move through this course, particularly in the [Experimental design] chapter. Step 6: Estimate the population parameter(s) Once we have a representative sample from a population we can calculate something called a point estimate of the population parameter. Remember, the population parameter is unknown; that’s why we collect samples. A point estimate is simply a number that represents our “best guess” at the true value of the parameter. For example, if we are interested in a population mean of a variable, then the obvious point estimate to use is the mean of the sample (this is just “the average” you learned how to calculate in school). By the way, people often just say/write “estimate” instead of “point estimate”, for the simple reason that using “point estimate” all the time is tedious. The exact terminology isn’t really all that important to be honest. We’ll mostly use the word “estimate” from now on. Step 7: Quantify the uncertainty of estimate(s) A point estimate is virtually useless on its own. Why? Because it is always derived from a limited sample of the wider population. Even if we are very careful about how we sample a population, and we collect a really big sample, there is no way to guarantee that the composition of the sample exactly matches that of the population. Why is this important? It means that any point estimate we derive from a sample will always be imperfect, in the sense that it won’t exactly match the true population value. So… there is always uncertainty associated with an estimate of a population parameter. What can we do about this? We have to find a way to quantify that uncertainty. This bit of the process can be tricky to understand. We’re going to spend a fair bit of time thinking about it in the [Sampling variation and standard error] chapter, so we’ll leave it there for now. Step 8: Answer the question! Once we have point estimates and measures of uncertainty we’re in a position to start answering questions. We have to be very careful about how we go about this though. Let’s say we want to answer a seemingly simple question, such as, “Are there more than 200 tonnes of carbon per hectare stored in the peatland of the Peak District?” We could go out and sample a number of sites, measure the stored carbon at each site, and then calculate the mean of these measurements. What can we conclude if that sample mean is 210 t h-1? Not much, at least not until we have a sense of how reliable that mean is likely to be. To answer our question, we have to know how to assess whether or not the difference we observe (210 - 200 = 10) was just a fluke. The tools we’ll learn about in this course are designed to answer a range of different kinds of scientific question. Nonetheless, they all boil down to the same basic question: Is the pattern I see ‘real’, or is it instead likely to be a result of chance variation? To tackle this, we combine point estimates and measures of uncertainty in various ways. The good news is that statistical software like R will do all the hard work for us. We just have to learn how to understand what is happening and interpret the results it gives us. 6.3 A simple example The best way to begin getting some sense of how all this fits together is by working through an example. We’ll finish this chapter by introducing an example that we’ll come back to in later chapters. We’ll just skim through steps 1-6 here. The final two steps are sufficiently tricky that they need their own chapters. Imagine we are working on a plant species that is phenotypically polymorphic. There are two different ‘morphs’, a purple morph and a green morph. We can depict this situation visually with a map showing where the purple and green plants are located on a hypothetical landscape: Figure 6.1: Stylised landscape showing a population of purple and green plants These idealised data were generated using a simulation in R. The details of how we did this aren’t important, but basically, we placed ‘individuals’ onto the landscape at random locations (every location is equally likely), and then assigned them purple morph status with a certain probability (we made them are green otherwise). We’ll come back to the probability we actually used in the next chapter. Let’s proceed as though this were a real situation… Step 1: Refine your questions, hypotheses and predictions Imagine we had previously been studying a neighbouring population that exhibits the same polymorphism. We’re fairly sure both populations were once connected, but habitat loss over the last few hundred years has significantly reduced gene flow between them. Our studies with the neighbouring population have shown that… The colour polymorphism is controlled by a single gene with two alleles: a recessive mutant allele (‘P’) confers the purple colour, and the dominant wild-type allele (‘G’) makes plants green. Population genetic studies have shown that the two alleles are present in a ratio of about 1:1. There seems to be no observable fitness difference between the two morphs in the neighbouring population. What’s more, about 25% of plants are purple, i.e. the alleles seem to be in Hardy-Weinberg equilibrium. These two observations indicate that there is no selection operating on the polymorphism (it’s ‘neutral’). Things are different in the new study population. The purple morph seems to be about as common as the green morph. What’s more, some preliminary work indicates that purple plants seem to produce more seeds than green plants. Our hypothesis is, therefore, that purple plants have a selective advantage in the new study population. The corresponding prediction is that the frequency of the purple morph will be greater than 25% in the new study population, as selection should be driving the ‘P’ allele to fixation. (This isn’t the strongest test of our hypothesis by the way. Really, we need to study allele and genotype frequencies, not just phenotypes. Sadly, since Brexit happened, the government has pulled the research funding for genetic research on plant polymorphism, so this is the best we can do) Step 2: Decide which population is important Our situation is made up, so questions about the statistical population are not hugely relevant to be honest. In reality, we would consider various factors, such as whether we can study the whole population or need to restrict ourselves to a smaller scale (e.g. to one sub-population). Working at a large scale should produce a more general result, but it could also present a significant logistical challenge. Step 3: Decide which variables to study This step is easy in this example. We could measure all kinds of different attributes of our plants—biomass, height, seed production, etc—but to study the polymorphism, we only need to collect information about the colour of different individuals. This means we are going to be working with a nominal (i.e. categorical) variable, which takes two values: ‘purple’ or ‘green’. Step 4: Decide which population parameters are relevant The prediction we want to test is about the purple morph frequency (or equivalently, the percentage, or proportion, of purple plants). Therefore, the relevant population parameter is the frequency of purple morphs in the wider population. We need to collect ‘data’ so that we can learn about this unknown quantity. Step 5: Gather a representative sample A representative sample here is one in which every individual on the landscape has the same probability of being sampled (i.e. a ‘random sample’). Gathering a random sample of organisms from across a landscape is surprisingly hard to do in reality, but it is at least easy to do in a simulation. Let’s seen what happens if we sample 20 plants at random… Figure 6.2: Sampling plants. Sampled plants are circled in red The new plot shows the original population of plants, only this time we’ve circled the sampled individuals in red. Step 6: Estimate the population parameter Estimating a frequency from a sample is simple enough. We can express a frequency in different ways. Let’s use a percentage. We found 13 green plants and 7 purple plants in our sample, which means our point estimate of the purple morph frequency is 35%. This is certainly greater than 25%—the value of observed in the original population—but it isn’t that far off. Maybe the purple plants aren’t at a selective advantage after all? Or maybe they are? We’ll eventually see how to use a statistical test to rigorously evaluate our prediction. First we need to learn a few more concepts. Time to learn about something called sampling error… "],
["sampling-error.html", "Chapter 7 Sampling error 7.1 Sampling error 7.2 Sampling distributions 7.3 The effect of sample size 7.4 The standard error 7.5 What is the point of all this!?", " Chapter 7 Sampling error In the previous chapter we introduced the idea that a point estimate of a population parameter will be imperfect, in the sense that it won’t exactly reflect the true value of that parameter. This uncertainty is always present, so it’s not enough to have just estimated something. We have to know about the uncertainty (i.e. the precision) of the estimate. We use the machinery of statistics to quantify this uncertainty. Once we have pinned down the uncertainty we can start to provide meaningful answers to our scientific questions. We will arrive at this ‘getting to the answer step’ in the next chapter. First we have to develop the uncertainty idea a bit more. We need to learn about sampling error, sampling distributions and standard errors. 7.1 Sampling error Let’s carry on with the plant polymorphism example from the previous chapter: the green-purple plant polymorphism. Skim back over the example if you can’t remember it, as you need to know what we’re trying to do for this chapter to be useful. So far, we had taken one sample of 20 plants from our hypothetical population and found the the frequency of purple plants in that sample was 35%. This is a point estimate of purple plant frequency based on a random sample of 20 plants. What happens if we repeat the same process, leading to a new, completely independent sample? Here’s a reminder of what the population looked like, along with a new sample highlighted with red circles: Figure 7.1: Plants sampled on the second occasion This time we ended up sampling 16 green plants and 4 purple plants, so our second estimate of the purple morph frequency is 20%. This is quite different from the first estimate. Notice that it is actually lower than that seen in the original study population. Our hypothesis that the purple morph will be more prevalent in the new study population is beginning to look a little shaky… Note that nothing about the study population changed between the first and second sample. What’s more, we used a completely reliable sampling scheme to generate these samples (you’ll have to trust us on that one). There was nothing biased or ‘incorrect’ about the way individuals were sampled—every individual had the same chance of being selected. The two different estimates of the purple morph frequency simply arise from chance variation in selection. This variation, which arises whenever we observe a sample instead of the whole population, has a special name. It is called the sampling error. (Another name for sampling error is ‘sampling variation’. Which one is better? Neither really. We tend to use both terms—‘sampling error’ and ‘sampling variation’—in this book because they are both widely used) Sampling error is the main reason why we have to use statistics. Any estimate you derive from a sample is affected by it. Sampling error is not really a property of any particular sample though. The form of sampling error in any given problem is a consequence of the population distribution of the variable(s) we’re studying, and the sampling method used to investigate this. That may seem a little cryptic now. Don’t worry, we will start to get a sense of what it really means in this chapter. 7.2 Sampling distributions We can develop our simple simulation example a bit more to explore the consequences of sampling error. However, rather than taking one sample at a time, we’ll use R to simulate thousands of independent samples. The number of plants sampled (‘n’) will always be 20. Here’s the important bit: every sample is drawn from the same population, i.e. the population parameter (purple morph frequency) will never change across samples. This means any variation we observe will be due to nothing more sampling error. Here is a summary of one such repeated simulation exercise: Figure 7.2: Distribution of number of purple morphs sampled (n = 20) This bar plot summarises the result from 100000 samples. In each sample, we took 20 individuals from our hypothetical population and calculated the number of purple morphs found. The bar plot shows the number of times we found 0, 1, 2, 3, … purple individuals, all the way up to the maximum possible (20). We could have converted these numbers to frequencies, but instead we’re just summarising the raw distribution of purple morph counts that we found. This distribution has a special name. It is called a sampling distribution. The sampling distribution is just the distribution we expect a particular estimate (or more generally, a ‘statistic’) to follow. In order to to work this out, we have to postulate values for the population parameters, and we have to know how the population was sampled. Rather than use mathematical reasoning, we used brute-force simulation to approximate the sampling distribution of purple morph counts that arises when we sample 20 individuals from our hypothetical population. What does the sampling distribution show? It shows us the range of outcomes we can expect when we repeat the same sampling process over and over again. The most common outcome is 8 purple morphs, which would yield an estimate of 8/20 = 40% for the purple morph frequency. This is the frequency that was actually used to simulate the data (we didn’t tell you that before). The population parameter we’re trying to learn about turns out to be the most common point estimate we should expect to see under repeated sampling. (So now we know the answer to question. The purple morph frequency is 40%. Of course we cheated though, because we used information from 1000s of samples. In the real world we only have one, limited sample.) The sampling distribution is the key to ‘doing statistics’. Look at the spread (dispersion) of the sampling distribution above. The range of outcomes is roughly 2 to 15, which corresponds to estimated frequencies of the purple morph in the range of 10-75%, because we sampled 20 individuals on each occasion. This tells us that when we sample only 20 individuals, the sampling error is expected to be quite large. Note that the sampling distribution we summarised above is only relevant for the case where 20 individuals are sampled, and the frequency of purple plants in the population is 40%. If we change either of those two things we would end up with a different sampling distribution. That’s what we meant when we said, “The form of sampling error in any given problem is a consequence of the population distribution of the variable(s) we’re studying, and the sampling method used to investigate this.” Once we know how to calculate the sampling distribution for a particular problem, we can start to make statements about sampling error (to quantify uncertainty), and we can begin to make meaningful comparisons to address scientific questions. We don’t have to work any of this out for ourselves — statisticians have done the hard work for us. 7.3 The effect of sample size One of the most important aspect of a sampling scheme is the sample size (denoted ‘n’). This is just the number of observations (individuals, objects, items, etc) in a sample. What happens when we change the sample size? We’ll carry on with the example to see how sample size influences the sampling distribution, and to understand why it matters. Let’s repeat the multiple sampling exercise, but this time do it with two different sample sizes. First we’ll use a sample size of 40 individuals, and then we’ll take a sample of 80 individuals each time. As before, we’ll take a total 100000 samples each time: Figure 7.3: Distribution of number of purple morphs sampled (n = 40) Figure 7.4: Distribution of number of purple morphs sampled (n = 80) What do these plots tell us about the effect of changing sample size? Notice that we plotted each of them over the full range of possible outcomes (the x axis runs from 0-40 and 0-80, respectively, in the first and second plot). This is so we can meaningfully compare the spread of each sampling distribution relative to the range of possible outcomes. The range of outcomes in the first plot (n = 40) is roughly 6 to 26, which corresponds to estimated frequencies of the purple morph in the range of 15-65%. The range of outcomes in the second plot (n = 80) is roughly 16 to 48, which corresponds to estimated frequencies in the range of 20-60%. The implications of this not so rigorous assessment are probably obvious. When we increase the sample size we can expect to encounter less sampling error. This makes intuitive sense: the composition of large sample should more closely approximate that of the true population than a small sample. How much data do we need to collect to accurately estimate a frequency? Here is the approximate sampling distribution of the purple morph frequency estimate when we sample 500 individuals: Figure 7.5: Distribution of number of purple morphs sampled (n = 500) Now the range of outcomes is about 160 to 240, corresponding to purple morph frequencies in the 32-48% range. This is a big improvement over the smaller samples that we just considered, but even with 500 individuals in a sample, we should still expect quite a lot of uncertainty in our estimate. The take home message is that you need a lot of data to reduce sampling error. 7.4 The standard error We’ve been fairly relaxed about how we quantified the spread of a sampling distribution up until this point. We just estimated the approximate range of purple morph counts “by eye”. This is fine for investigating general patterns, but to make rigorous comparisons, we really need a quantitative measure of this variability. This is called the standard error. The standard error is actually quite a simple idea, though its definition often causes confusion. Here is that definition: a standard error is the standard deviation of the sampling distribution of an estimate, like a mean or a frequency. Don’t worry if that makes absolutely no sense. The key point is that it is a standard deviation, so it a measure of the spread, or dispersion, of a distribution. The distribution in the case of a standard error is the sampling distribution of some kind of estimate. (It is common to use a shorthand abbreviations such “SE”, “S.E.”, “se” or “s.e.” in place of ‘standard error’ when referring to the standard error in text.) We can use a simulation in R to calculate the expected standard error of an estimate of purple morph frequency. In order to do this we have to specify the value of the population frequency, and we have to decide what sample size we want to evaluate. Let’s find the expected standard error when the purple morph frequency is 40% and the sample size is 80. First we set up the simulation by assigning values to different variables to control what the simulation does: purple_prob &lt;- 0.4 sample_size &lt;- 80 n_samples &lt;- 100000 The value of purple_prob is the probability a plant will be purple (0.4 — R doesn’t like percentages), the value of sample_size is the sample size for each sample, and the value of n_samples is the number of independent samples we’ll take. That’s simple enough. raw_samples &lt;- rbinom(n = n_samples, size = sample_size, prob = purple_prob) percent_samples &lt;- 100 * raw_samples / sample_size You don’t have to understand how this works, but if you did A-level statistics you might be able to guess what the rbinom function is doing. Honestly though, the R code isn’t important here. We’re just showing it to you to demonstrate that seemingly complex simulations are often easy to do in R. It is more or less the same code we used to generate those plots above (the only difference is that this time we converted the numbers into proportions). The result is what matters. We simulated the percentage of purple morph individuals found in 100000 samples of 20 individuals, assuming the purple morph frequency is always 40%. The results are stored the result in a vector called percent_samples. Here are the first 50 values of that vector: head(percent_samples, 50) ## [1] 42.50 37.50 36.25 30.00 35.00 35.00 51.25 35.00 41.25 36.25 32.50 ## [12] 37.50 32.50 38.75 37.50 36.25 33.75 35.00 42.50 41.25 46.25 35.00 ## [23] 42.50 45.00 43.75 40.00 41.25 52.50 43.75 38.75 38.75 47.50 42.50 ## [34] 40.00 43.75 42.50 36.25 42.50 43.75 43.75 35.00 42.50 35.00 37.50 ## [45] 41.25 40.00 45.00 40.00 41.25 38.75 These numbers are all part of the sampling distribution of morph frequency estimates. So… how to calculate the standard error? This is the standard deviation of these numbers, so we just use the sd function: sd(percent_samples) ## [1] 5.494687 What is this useful? The standard error gives us a standard means to compare the variability we expect to see, or the variability we actually see, in different sampling distributions. As long as the sampling distribution is ‘well-behaved’, then, roughly speaking, most estimates (~95%) can be expected to lie in a range of about four standard errors. If you’re not convinced, look at the second bar plot we produced above (where the sample size = 80, and the purple morph frequency = 40%). What is the approximate range of simulated values? How close is this to \\(4 \\times 5.5\\)? Pretty close we think… So in summary, the standard error gives us a way to quantify how much variability we expect to see in a sampling distributions. We said in the previous chapter (Learning from data) that a point estimate is useless without some kind of associated measure of uncertainty. A standard error is one such measure. 7.5 What is the point of all this!? By this point you might (quite reasonably) be wondering why we have spent so much time looking at properties of repeated samples from a population. After all, when we collect data in the real world we’ll only have a single sample to work with. We can’t just keep collecting more and more data. We also won’t know anything about the population parameter of interest. This lack of knowledge is the reason for collecting the data in the first place! The short answer to this question is that before we can start to use frequentist statistics—remember, that’s our ultimate goal—we need to have a sense of… how point estimates behave under repeated sampling (i.e. sampling distributions), and how ‘sampling error’ and ‘standard error’ relate to sampling distributions. Once we understand these links, we’re able to start exploring the techniques that underlie frequentist statistics. That’s what we’ll do in the next block of work… "]
]
